
# default basic settings for UCAgent
lang: "zh"

model_type: openai

openai:
  model_name: "<your_chat_model_name>"
  openai_api_key: "your_api_key"
  openai_api_base: "http://<your_chat_model_url>/v1"
  model_kwargs:
    stop: ["."]

embed:
  model_name: "<your_embedding_model_name>"
  openai_api_key: "your_api_key"
  openai_api_base: "http://<your_embedding_model_url>/v1"
  dims: 4096

# This is the setting for conversation summary
# Adjust the max_tokens and max_summary_tokens according to your needs and model capabilities
# Reference doc: https://langchain-ai.lang.chat/langmem/reference/short_term/#langmem.short_term.SummarizationNode
# if use_trim_mode is false (summarization mode):
#   Param: max_tokens, suggested 50% of the model's context length
# if use_trim_mode is true (trim mode):
#   Param: max_tokens, suggested 80% of the model's context length
conversation_summary:
  max_tokens: 51200   # default 50k tokens for 128k context model
  max_summary_tokens: 1024
  use_trim_mode: true # default use trim mode to manage conversation history
  max_keep_msgs: 200  # max messages to keep in memory, older messages will be removed (not the messages to LLM)

template: unity_test

un_write_dirs:
  - "{DUT}"
  - "Guide_Doc"
write_dirs:
  - "{OUT}"

tools:
  RunTestCases:
    test_dir: "{OUT}/tests"
