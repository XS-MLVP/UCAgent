
# default basic settings for UCAgent
lang: "zh"

# Model support: openai, anthropic, google_genai
model_type: openai

openai:
  model_name: "$(OPENAI_MODEL: <your_chat_model_name>)"
  openai_api_key: "$(OPENAI_API_KEY: [your_api_key])"
  openai_api_base: "$(OPENAI_API_BASE: http://<your_chat_model_url>/v1)"
  model_kwargs:
    stop: ["."]

# export ANTHROPIC_API_KEY="your-api-key"
anthropic:
  model: "$(ANTHROPIC_MODEL: claude-3-7-sonnet-20250219)"

# export GOOGLE_GENAI_API_KEY="your-api-key"
google_genai:
  model: "$(GOOGLE_GENAI_MODEL: gemini-2.5-pro)"

embed:
  model_name: "$(EMBED_MODEL: <your_embedding_model_name>)"
  openai_api_key: "$(EMBED_OPENAI_API_KEY: [your_api_key])"
  openai_api_base: "$(EMBED_OPENAI_API_BASE: http://<your_embedding_model_url>/v1)"
  dims: 4096

# This is the setting for conversation summary
# Adjust the max_tokens and max_summary_tokens according to your needs and model capabilities
# Reference doc: https://langchain-ai.lang.chat/langmem/reference/short_term/#langmem.short_term.SummarizationNode
# if use_uc_mode is false (summarization mode):
#   Param: max_tokens, suggested 50% of the model's context length
# if use_uc_mode is true (unity chip summarization and trim mode):
#   Param: max_tokens is not used
#   Param: max_summary_tokens, suggested 10% of the model's context length
conversation_summary:
  max_tokens: 51200    # default 50k tokens for 128k context model
  max_summary_tokens: 1024
  max_keep_msgs: 100   # max messages to keep in memory, older messages will be removed (not the messages to LLM)
  use_uc_mode: true    # default use uc mode to manage conversation history
  tail_keep_msgs: 10   # when use_uc_mode is true, keep the last N messages to the LLM no matter what

rate_limiter:
  enabled: false
  # The following settings are used when rate_limiter.enabled is true
  requests_per_second: 10    # default 10 req/s
  check_every_n_seconds: 0.1 # default 0.1s, wake up every 100 ms to check whether allowed to make a request
  max_bucket_size: 1         # default 1, controls the maximum burst size

template: unity_test

un_write_dirs:
  - "{DUT}"
  - "Guide_Doc"
write_dirs:
  - "{OUT}"

tools:
  RunTestCases:
    test_dir: "{OUT}/tests"
