
# default basic settings for UCAgent
lang: "zh"

# init commands to run at the start of the agent
init_cmds: []

# backend
backend:
  key_name: "langchain"  # options: langchain, claude, opencode, copilot, etc.
  langchain:
    clss: ucagent.abackend.langchain.UCAgentLangChainBackend
  claude:
    clss: ucagent.abackend.UCAgentCmdLineBackend
    args:
      cli_cmd_new: "claude {UC_ENV_CMD_BACKEND_EX_ARGS} --dangerously-skip-permissions -p < {MSG_FILE}"
      cli_cmd_ctx: "claude {UC_ENV_CMD_BACKEND_EX_ARGS} --dangerously-skip-permissions -c -p < {MSG_FILE}"
      pre_bash_cmd:
        - "mkdir -p {CWD}/.claude/"
        - "cp ~/.claude/.mcp.json {CWD}/.mcp.json" # Add a project-scoped server claude mcp add --scope project ...., then backup the config to ~/.claude/.mcp.json
        - "sed -i \"s/5000\/mcp/{PORT}\/mcp/\" {CWD}/.mcp.json" # modify the mcp port (5000) to the configured port
  opencode:
    clss: ucagent.abackend.UCAgentCmdLineBackend
    args:
      cli_cmd_new: "opencode run {UC_ENV_CMD_BACKEND_EX_ARGS} < {MSG_FILE}"
      cli_cmd_ctx: "opencode run {UC_ENV_CMD_BACKEND_EX_ARGS} -c < {MSG_FILE}"
      pre_bash_cmd:
        - "cp ~/.config/opencode/opencode.json {CWD}/opencode.json" # You need to setup opencode config first
        - "sed -i \"s/5000\/mcp/{PORT}\/mcp/\" {CWD}/opencode.json" # modify the server port (5000) to the configured port
  copilot:
    clss: ucagent.abackend.UCAgentCmdLineBackend
    args:
      cli_cmd_new: "copilot {UC_ENV_CMD_BACKEND_EX_ARGS} --allow-all-tools < {MSG_FILE}"
      cli_cmd_ctx: "copilot {UC_ENV_CMD_BACKEND_EX_ARGS} --allow-all-tools --continue < {MSG_FILE}"
      pre_bash_cmd:
        - "mkdir -p {CWD}/.copilot/"
        - "cp ~/.copilot/mcp-config.json {CWD}/.copilot/mcp-config.json"
        - "sed -i \"s/5000\/mcp/{PORT}\/mcp/\" {CWD}/.copilot/mcp-config.json" # modify the mcp port (5000) to the configured port
  qwen:
    clss: ucagent.abackend.UCAgentCmdLineBackend
    args:
      cli_cmd_new: "qwen {UC_ENV_CMD_BACKEND_EX_ARGS} -y -p < {MSG_FILE}"
      cli_cmd_ctx: "qwen {UC_ENV_CMD_BACKEND_EX_ARGS} -y -c -p < {MSG_FILE}"
      pre_bash_cmd:
        - "mkdir -p {CWD}/.qwen/"
        - "cp ~/.qwen/settings.json {CWD}/.qwen/settings.json"
        - "sed -i \"s/5000\/mcp/{PORT}\/mcp/\" {CWD}/.qwen/settings.json" # modify the mcp port (5000) to the configured port
  kilo:
    clss: ucagent.abackend.UCAgentCmdLineBackend
    args:
      cli_cmd_new: "kilo run {UC_ENV_CMD_BACKEND_EX_ARGS} < {MSG_FILE}"
      cli_cmd_ctx: "kilo run {UC_ENV_CMD_BACKEND_EX_ARGS} -c < {MSG_FILE}"
      pre_bash_cmd:
        - "mkdir -p {CWD}/.kilocode/"
        - "cp ~/.kilocode/cli/global/settings/mcp_settings.json {CWD}/.kilocode/mcp.json"
        - "sed -i \"s/5000\/mcp/{PORT}\/mcp/\" {CWD}/.kilocode/mcp.json" # modify the mcp port (5000) to the configured port

mcp_server:
  host: 127.0.0.1
  port: 5000

# Model support: openai, anthropic, google_genai
model_type: openai

openai:
  model_name: "$(OPENAI_MODEL: <your_chat_model_name>)"
  openai_api_key: "$(OPENAI_API_KEY: [your_api_key])"
  openai_api_base: "$(OPENAI_API_BASE: http://<your_chat_model_url>/v1)"
  #temperature: $(OPENAI_TEMPERATURE: 0.7)
  #top_p: $(OPENAI_TOP_P: 0.9)
  model_kwargs:
    stop: ["."]

# export ANTHROPIC_API_KEY="your-api-key"
anthropic:
  model: "$(ANTHROPIC_MODEL: claude-3-7-sonnet-20250219)"

# export GOOGLE_GENAI_API_KEY="your-api-key"
google_genai:
  model: "$(GOOGLE_GENAI_MODEL: gemini-2.5-pro)"

embed:
  model_name: "$(EMBED_MODEL: <your_embedding_model_name>)"
  openai_api_key: "$(EMBED_OPENAI_API_KEY: [your_api_key])"
  openai_api_base: "$(EMBED_OPENAI_API_BASE: http://<your_embedding_model_url>/v1)"
  dims: 4096

langfuse:
  enable: $(ENABLE_LANGFUSE, false)
  public_key: $(LANGFUSE_PUBLIC_KEY, <YOUR_LANGFUSE_PUBLIC_KEY>)
  secret_key: $(LANGFUSE_SECRET_KEY, <YOUR_LANGFUSE_SECRET_KEY>)
  base_url: $(LANGFUSE_URL, http://localhost:3000)

# This is the setting for conversation summary
# Adjust the max_tokens and max_summary_tokens according to your needs and model capabilities
# Reference doc: https://langchain-ai.lang.chat/langmem/reference/short_term/#langmem.short_term.SummarizationNode
conversation_summary:
  context_management_strategy: TrimAndSummaryMiddleware    # default use TrimAndSummaryMiddleware strategy to manage conversation history
  max_tokens: $(SUMMARY_MAX_CTX_TOKEN: 51200)    # default 50k tokens for 128k context model
  max_summary_tokens: $(SUMMARY_MAX_SUM_TOKEN: 1024)  # suggested 10% of the model's context length
  max_keep_msgs: $(SUMMARY_MAX_KEEP_MSG: 100)    # max messages to keep in memory, older messages will be removed (not the messages to LLM)
  tail_keep_msgs: $(SUMMARY_TAIL_KEEP_MSG: 10)   # keep the last N messages to the LLM no matter what

rate_limiter:
  enabled: $(ENABLE_LLM_RATE_LIMIT: false)
  # The following settings are used when rate_limiter.enabled is true
  requests_per_second: $(LLM_MAX_RPS: 10)    # default 10 req/s
  check_every_n_seconds: 0.1 # default 0.1s, wake up every 100 ms to check whether allowed to make a request
  max_bucket_size: 1         # default 1, controls the maximum burst size

template: unity_test

un_write_dirs:
  - "{DUT}"
  - "Guide_Doc"
write_dirs:
  - "{OUT}"

tools:
  RunTestCases:
    test_dir: "{OUT}/tests"
  ignore_tools: ["WorkDiff", "WorkCommit", "RunBashCommand"] # List of tool names to ignore
  selected_tools: []     # List of tool names to enable, if empty, all tools are enabled except those in ignore_tools

# Tool call timeout
call_time_out: 300  # seconds

# TUI layout settings
tui:
  task_width: 84
  console_height: 13
  status_height: 7

loop_settings:
  max_loop_retry: 10       # maximum number of retries for a loop
  retry_delay_start: 5     # initial delay time in seconds before retrying
  retry_delay_end: 10      # maximum delay time in seconds before retrying
  loop_alive_time: 120     # time in seconds to consider a loop alive before resetting retry count

hooks:
  continue: >
    You have not completed all the tasks yet. Please continue. Use the `Check` and `Complete` tools to determine whether you have finished the current stage's tasks.
    Please verify the results of the `Check` and `Complete` tool calls, and adjust your work based on the feedback until all `Check` and `Complete` calls are passed.
    If a timeout error occurs, please increase the timeout parameter and retry, or use the RunTestCases tool to identify any stuck test cases and fix them.

  cagent_init: >
    Please get your role information and basic guidance through tool `RoleInfo`, and then complete the task.
    Please make sure to use the tool `ReadTextFile` to read the file, so that UCAgent can know which files you have read.

  quit: "/quit"

  exit: "/exit"


vmanager:
  llm_suggestion:
    check_fail_refinement:
      enable: $(ENABLE_LLM_FAIL_SUGGESTION: false)
      clss: ucagent.stage.llm_suggestion.OpenAILLMFailSuggestion
      args:
        model_name: $(FAIL_SUGGESTION_MODEL: <your_chat_model_name>)
        openai_api_key: $(FAIL_SUGGESTION_API_KEY: <your_api_key>)
        openai_api_base: $(FAIL_SUGGESTION_API_BASE: http://<your_chat_model_url>/v1)
        min_fail_count: $(FAIL_SUGGESTION_MFCOUNT: 3)
      system_prompt: >
        You are a professional testing assistant. An LLM expert is testing a Device Under Test (DUT) (Current DUT is `{DUT}`) based on the high-level task requirements.
        Your job is to analyze the current task fail information, identify the reason
        why the current stage of 'Check' did not pass, and then provide him precise recommendations to pass it.
        In your recommendations:
        1. Key information from the test data must be retained, such as important details from std error.
        2. The proposed solutions should be clear and actionable.
        3. Prioritize suggestions that have the most significant impact on passing the Checker.
        4. Ensure that your suggestions are actionable and easy to understand.
        5. Avoid suggesting changes that are too complex or require significant refactoring unless absolutely necessary.
        6. Do not include any thought process or analysis in your final suggestion, only provide the actionable suggestion.
        Note:
          Your tool has a limited call quota, so make sure to use them wisely.
          The goal of the high-level task is to identify and document DUT bugs, not to make reasonable fail cases pass.
          The tasks and test information is dynamic, you should keep updating your analysis and suggestions based on the latest test information.
          You are not care about the bugs in the DUT, only focus on helping the LLM expert to pass the 'Check' tasks.
      suggestion_prompt: >
        Now, the tasks corresponding 'Check' did not pass.
        Please analyze the above test information, then provide the precise recommendations to pass it.
      bypass_stages: []
      target_stages: []
      default_apply_all_stages: true
    check_pass_refinement:
      enable: $(ENABLE_LLM_PASS_SUGGESTION: false)
      clss: ucagent.stage.llm_suggestion.OpenAILLMPassSuggestion
      args:
        model_name: $(PASS_SUGGESTION_MODEL: <your_chat_model_name>)
        openai_api_key: $(PASS_SUGGESTION_API_KEY: <your_api_key>)
        openai_api_base: $(PASS_SUGGESTION_API_BASE: http://<your_chat_model_url>/v1)
      system_prompt: >
        You are a verification expert, highly skilled at identifying issues in others' verification work, with extremely high standards for the quality of verification projects.
        Another LLM verification expert is currently verifying the DUT ({DUT}). At the end of each critical stage, it requires your evaluation of the results.
        Only after you confirm that there are no issues can it proceed to subsequent stages. Therefore, your role is crucial.
        Your specific responsibilities are as follows:
        (1) Clarify the specific tasks for the current stage and define evaluation criteria based on the stage's requirements.
        (2) If issues are identified, provide clear and actionable suggestions for improvement.
        (3) Ensure the verification results align with the requirements outlined in the reference documentation.
        (4) If no issues are found, call tool `ApproveStagePass` to approve the current stage.
        (5) You must prevent the LLM verification expert from focusing solely on passing `Checker` inspections at the expense of thoroughly and properly completing the actual verification work.
        Important Notes:
        • For verification work, you place great emphasis on ensuring no functional points are missing.
        • Checkpoints must be clearly described and highly actionable.
        • Your suggestions should be concise and to the point, avoiding unnecessary length.
        • You can use tool `StageDiff` to help you analyze the differences in the work results before and after the current stage.
        • You can use tool `StageCommit` to help you commit the current stage's work results for better analysis.
      suggestion_prompt: >
        Now, the LLM verification expert has completed the tasks for the current stage.
        Please evaluate whether the tasks have been completed properly based on the above requirements.
        If you find any issues, please provide clear and actionable suggestions for improvement.
        You Must use `ApproveStagePass` to approve or not approve the current stage to be passed.
      bypass_stages: []
      target_stages: []
      default_apply_all_stages: true


# history version control settings
hist_ignore_pattern:
  - ".pytest_cache"
  - "__pycache__"
  - "*.pyc"
  - ".*"
  - "data"
  - ".git"
